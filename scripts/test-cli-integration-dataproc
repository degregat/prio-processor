#!/bin/bash
set -eou pipefail
set -x

cd "$(dirname "$0")/.."
scripts/create-folder

# test parameters for data size
N_DATA=${N_DATA:-128}
N_ROWS=${N_ROWS:-1000}
SCALE=${SCALE:-1}
BATCH_ID=${BATCH_ID:-"test"}

# parameters for computational performance, exported for dataproc script
export NUM_WORKERS=${NUM_WORKERS:-0}
export MACHINE_TYPE=${MACHINE_TYPE:-"n1-standard-4"}

timestamp=$(python3 -c "import datetime as dt, re; print(re.sub('[^\d+]', '', str(dt.datetime.now()))[:14])")
BENCHMARK_NAME="${BATCH_ID}-${N_DATA}-$((N_ROWS * SCALE))-${MACHINE_TYPE}-${NUM_WORKERS}-${timestamp}"
echo "running benchmark $BENCHMARK_NAME"

# set up the storage locations for working and artifact
BUCKET=${BUCKET?"must include a valid bucket"}
PREFIX=${PREFIX:-"data"}

ARTIFACT_BUCKET="$BUCKET/$PREFIX/benchmark/$BENCHMARK_NAME"
# relative to the current working directory
WORKING_FOLDER=${WORKING_FOLDER:-$PREFIX/$BENCHMARK_NAME}
WORKING_BUCKET="$BUCKET/$PREFIX/working/$BENCHMARK_NAME"
CLIENT_BUCKET=${CLIENT_BUCKET:-"$WORKING_BUCKET/client"}
SERVER_A_BUCKET=${SERVER_A_BUCKET:-"$WORKING_BUCKET/server_a"}
SERVER_B_BUCKET=${SERVER_B_BUCKET:-"$WORKING_BUCKET/server_b"}

# set up so `gcloud dataproc` commands don't require `--region` option
export REGION=${REGION:-us-west1}
export CLOUDSDK_DATAPROC_REGION=$REGION

function run_spark_submit() {
    submit $BENCHMARK_NAME $WORKING_BUCKET/bootstrap "$@"
}

function collect_history() {
    # collect some history for benchmarking analysis and performance debugging
    local artifact_bucket=$ARTIFACT_BUCKET
    local cluster_id=$BENCHMARK_NAME
    local working_folder=$WORKING_FOLDER

    mkdir -p $working_folder/logs
    gsutil ls -rl "$WORKING_BUCKET/**" >$working_folder/logs/bucket-listing.txt
    gcloud beta dataproc clusters describe $cluster_id --format json >$working_folder/logs/dataproc-clusters-describe.json
    gcloud beta dataproc jobs list --cluster $cluster_id --format json >$working_folder/logs/dataproc-jobs-list.json
    gsutil -m rsync -r $working_folder/ $artifact_bucket/

    # copy spark history and yarn logs
    cluster_uuid=$(jq -r '.clusterUuid' $working_folder/logs/dataproc-clusters-describe.json)
    gsutil -m rsync -r "gs://dataproc-temp*/${cluster_uuid}" $artifact_bucket/logs

    # copy the raw and processed data for potential analysis
    gsutil -m rsync -r $WORKING_BUCKET/client/ $artifact_bucket/client/
    gsutil -m rsync -r $SERVER_A_BUCKET/processed/ $artifact_bucket/server_a/processed/
    gsutil -m rsync -r $SERVER_B_BUCKET/processed/ $artifact_bucket/server_b/processed/
}

# check local installion of prio commandline
prio --help

[[ $BUCKET == "gs://"* ]]

# check for appropriate access
gsutil ls $BUCKET

# make the working folder
mkdir -p $WORKING_FOLDER

START_STAGE=${START_STAGE:-0}
SKIP_GENERATE=${SKIP_GENERATE:-"false"}
if ((START_STAGE > 0)); then
    SKIP_GENERATE="true"
fi

if [[ "$SKIP_GENERATE" == "false" ]]; then
    # generate some configuration
    key_a=$(prio keygen)
    key_b=$(prio keygen)
    shared=$(prio shared-seed)

    echo $key_a >"$WORKING_FOLDER/server_a_keys.json"
    echo $key_b >"$WORKING_FOLDER/server_b_keys.json"
    echo $shared >"$WORKING_FOLDER/shared_seed.json"
    cat <<EOF >"$WORKING_FOLDER/config.json"
{
    "n_data": $N_DATA,
    "batch_id": "$BATCH_ID",
    "n_rows": $N_ROWS,
    "scale": $SCALE,
    "machine_type": "$MACHINE_TYPE",
    "num_workers": $NUM_WORKERS
}
EOF
else
    key_a=$(jq '.' $WORKING_FOLDER/server_a_keys.json)
    key_b=$(jq '.' $WORKING_FOLDER/server_b_keys.json)
    shared=$(jq '.' $WORKING_FOLDER/shared_seed.json)
fi

N_DATA=$(jq -r ".n_data" $WORKING_FOLDER/config.json)
BATCH_ID=$(jq -r ".batch_id" $WORKING_FOLDER/config.json)
SHARED_SECRET=${SHARED_SECRET:-$(jq -r ".shared_seed" <<<$shared)}
SERVER_A_PUBLIC_KEY=${SERVER_A_PUBLIC_KEY:-$(jq -r ".public_key" <<<$key_a)}
SERVER_B_PUBLIC_KEY=${SERVER_B_PUBLIC_KEY:-$(jq -r ".public_key" <<<$key_b)}
SERVER_A_PRIVATE_KEY=${SERVER_A_PRIVATE_KEY:-$(jq -r ".private_key" <<<$key_a)}
SERVER_B_PRIVATE_KEY=${SERVER_B_PRIVATE_KEY:-$(jq -r ".private_key" <<<$key_b)}

# shellcheck source=bin/dataproc
source bin/dataproc

# start a dataproc cluster if one doesn't already exist
if ! gcloud dataproc clusters describe $BENCHMARK_NAME; then
    bootstrap $WORKING_BUCKET/bootstrap
    create_cluster $BENCHMARK_NAME $WORKING_BUCKET
    function cleanup() {
        delete_cluster ${BENCHMARK_NAME}
    }
    trap cleanup EXIT
fi

if [[ "$SKIP_GENERATE" == "false" ]]; then
    time run_spark_submit generate \
        --n-data ${N_DATA} \
        --batch-id ${BATCH_ID} \
        --public-key-hex-internal ${SERVER_A_PUBLIC_KEY} \
        --public-key-hex-external ${SERVER_B_PUBLIC_KEY} \
        --output ${CLIENT_BUCKET} \
        --n-rows ${N_ROWS} \
        --scale ${SCALE}

    gsutil -m rsync -r -d ${CLIENT_BUCKET}/server_id=a/ ${SERVER_A_BUCKET}/raw/
    gsutil -m rsync -r -d ${CLIENT_BUCKET}/server_id=b/ ${SERVER_B_BUCKET}/raw/
fi

###########################################################
# verify1
###########################################################

if ((START_STAGE < 2)); then
    time run_spark_submit verify1 \
        --n-data ${N_DATA} \
        --batch-id ${BATCH_ID} \
        --server-id A \
        --private-key-hex ${SERVER_A_PRIVATE_KEY} \
        --shared-secret $SHARED_SECRET \
        --public-key-hex-internal ${SERVER_A_PUBLIC_KEY} \
        --public-key-hex-external ${SERVER_B_PUBLIC_KEY} \
        --input ${SERVER_A_BUCKET}/raw \
        --output ${SERVER_A_BUCKET}/intermediate/internal/verify1

    # NOTE: we ensure these buckets are completely synchronized, otherwise it is
    # possible to for the job to fail during the publishing step.
    gsutil -m rsync -r -d \
        ${SERVER_A_BUCKET}/intermediate/internal/verify1/ \
        ${SERVER_B_BUCKET}/intermediate/external/verify1/

    time run_spark_submit verify1 \
        --n-data ${N_DATA} \
        --batch-id ${BATCH_ID} \
        --server-id B \
        --private-key-hex ${SERVER_B_PRIVATE_KEY} \
        --shared-secret $SHARED_SECRET \
        --public-key-hex-internal ${SERVER_B_PUBLIC_KEY} \
        --public-key-hex-external ${SERVER_A_PUBLIC_KEY} \
        --input ${SERVER_B_BUCKET}/raw \
        --output ${SERVER_B_BUCKET}/intermediate/internal/verify1

    gsutil -m rsync -r -d \
        ${SERVER_B_BUCKET}/intermediate/internal/verify1/ \
        ${SERVER_A_BUCKET}/intermediate/external/verify1/
fi

###########################################################
# verify2
###########################################################

if ((START_STAGE < 3)); then
    time run_spark_submit verify2 \
        --n-data ${N_DATA} \
        --batch-id ${BATCH_ID} \
        --server-id A \
        --private-key-hex ${SERVER_A_PRIVATE_KEY} \
        --shared-secret $SHARED_SECRET \
        --public-key-hex-internal ${SERVER_A_PUBLIC_KEY} \
        --public-key-hex-external ${SERVER_B_PUBLIC_KEY} \
        --input ${SERVER_A_BUCKET}/raw \
        --input-internal ${SERVER_A_BUCKET}/intermediate/internal/verify1 \
        --input-external ${SERVER_A_BUCKET}/intermediate/external/verify1 \
        --output ${SERVER_A_BUCKET}/intermediate/internal/verify2/

    gsutil -m rsync -r -d \
        ${SERVER_A_BUCKET}/intermediate/internal/verify2/ \
        ${SERVER_B_BUCKET}/intermediate/external/verify2/

    time run_spark_submit verify2 \
        --n-data ${N_DATA} \
        --batch-id ${BATCH_ID} \
        --server-id B \
        --private-key-hex ${SERVER_B_PRIVATE_KEY} \
        --shared-secret $SHARED_SECRET \
        --public-key-hex-internal ${SERVER_B_PUBLIC_KEY} \
        --public-key-hex-external ${SERVER_A_PUBLIC_KEY} \
        --input ${SERVER_B_BUCKET}/raw \
        --input-internal ${SERVER_B_BUCKET}/intermediate/internal/verify1/ \
        --input-external ${SERVER_B_BUCKET}/intermediate/external/verify1/ \
        --output ${SERVER_B_BUCKET}/intermediate/internal/verify2/

    gsutil -m rsync -r -d \
        ${SERVER_B_BUCKET}/intermediate/internal/verify2/ \
        ${SERVER_A_BUCKET}/intermediate/external/verify2/
fi
###########################################################
# aggregate
###########################################################

if ((START_STAGE < 4)); then
    time run_spark_submit aggregate \
        --n-data ${N_DATA} \
        --batch-id ${BATCH_ID} \
        --server-id A \
        --private-key-hex ${SERVER_A_PRIVATE_KEY} \
        --shared-secret $SHARED_SECRET \
        --public-key-hex-internal ${SERVER_A_PUBLIC_KEY} \
        --public-key-hex-external ${SERVER_B_PUBLIC_KEY} \
        --input ${SERVER_A_BUCKET}/raw \
        --input-internal ${SERVER_A_BUCKET}/intermediate/internal/verify2 \
        --input-external ${SERVER_A_BUCKET}/intermediate/external/verify2 \
        --output ${SERVER_A_BUCKET}/intermediate/internal/aggregate

    gsutil -m rsync -r -d \
        ${SERVER_A_BUCKET}/intermediate/internal/aggregate/ \
        ${SERVER_B_BUCKET}/intermediate/external/aggregate/

    time run_spark_submit aggregate \
        --n-data ${N_DATA} \
        --batch-id ${BATCH_ID} \
        --server-id B \
        --private-key-hex ${SERVER_B_PRIVATE_KEY} \
        --shared-secret $SHARED_SECRET \
        --public-key-hex-internal ${SERVER_B_PUBLIC_KEY} \
        --public-key-hex-external ${SERVER_A_PUBLIC_KEY} \
        --input ${SERVER_B_BUCKET}/raw \
        --input-internal ${SERVER_B_BUCKET}/intermediate/internal/verify2 \
        --input-external ${SERVER_B_BUCKET}/intermediate/external/verify2 \
        --output ${SERVER_B_BUCKET}/intermediate/internal/aggregate

    gsutil -m rsync -r -d \
        ${SERVER_B_BUCKET}/intermediate/internal/aggregate/ \
        ${SERVER_A_BUCKET}/intermediate/external/aggregate/
fi

###########################################################
# publish
###########################################################

time run_spark_submit publish \
    --n-data ${N_DATA} \
    --batch-id ${BATCH_ID} \
    --server-id A \
    --private-key-hex ${SERVER_A_PRIVATE_KEY} \
    --shared-secret $SHARED_SECRET \
    --public-key-hex-internal ${SERVER_A_PUBLIC_KEY} \
    --public-key-hex-external ${SERVER_B_PUBLIC_KEY} \
    --input-internal ${SERVER_A_BUCKET}/intermediate/internal/aggregate/*.json \
    --input-external ${SERVER_A_BUCKET}/intermediate/external/aggregate/*.json \
    --output ${SERVER_A_BUCKET}/processed

gsutil -m rsync -r -d ${SERVER_A_BUCKET}/processed/ working/server_a/processed/
jq '.' working/server_a/processed/*.json
[[ $(jq ".error" working/server_a/processed/*.json) -eq 0 ]]

time run_spark_submit publish \
    --n-data ${N_DATA} \
    --batch-id ${BATCH_ID} \
    --server-id B \
    --private-key-hex ${SERVER_B_PRIVATE_KEY} \
    --shared-secret $SHARED_SECRET \
    --public-key-hex-internal ${SERVER_B_PUBLIC_KEY} \
    --public-key-hex-external ${SERVER_A_PUBLIC_KEY} \
    --input-internal ${SERVER_B_BUCKET}/intermediate/internal/aggregate/*.json \
    --input-external ${SERVER_B_BUCKET}/intermediate/external/aggregate/*.json \
    --output ${SERVER_B_BUCKET}/processed

gsutil -m rsync -r -d ${SERVER_B_BUCKET}/processed/ working/server_b/processed/
jq '.' working/server_b/processed/*.json
[[ $(jq ".error" working/server_b/processed/*.json) -eq 0 ]]

collect_history
