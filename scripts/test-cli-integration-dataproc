#!/bin/bash
set -eou pipefail
set -x

cd "$(dirname "$0")/.."
scripts/create-folder

export NUM_WORKERS=${NUM_WORKERS:-0}
export MACHINE_TYPE=${MACHINE_TYPE:-"n1-standard-4"}
BUCKET=${BUCKET?"must include a valid bucket"}
PREFIX=${PREFIX:-"test-integration-dataproc"}
WORKING_BUCKET="$BUCKET/$PREFIX"
ARTIFACT_BUCKET=${ARTIFACT_BUCKET:-$BUCKET/prio-processor-benchmarks}
CLIENT_BUCKET=${CLIENT_BUCKET:-"$WORKING_BUCKET/client"}
SERVER_A_BUCKET=${SERVER_A_BUCKET:-"$WORKING_BUCKET/server_a"}
SERVER_B_BUCKET=${SERVER_B_BUCKET:-"$WORKING_BUCKET/server_b"}
DATAPROC_CLUSTER_ID=${DATAPROC_CLUSTER_ID:-""}

export REGION=${REGION:-us-west1}
export CLOUDSDK_DATAPROC_REGION=$REGION

[[ $BUCKET == "gs://"* ]]
# check for appropriate access
gsutil ls $BUCKET

# shellcheck source=bin/dataproc
source bin/dataproc

# start a dataproc cluster if one doesn't already exist
if [[ -z "$DATAPROC_CLUSTER_ID" ]]; then
    timestamp=$(python3 -c "import datetime as dt, re; print(re.sub('[^\d+]', '', str(dt.datetime.now())))")
    DATAPROC_CLUSTER_ID="test-prio-processor-${timestamp}"
    bootstrap $WORKING_BUCKET
    create_cluster $DATAPROC_CLUSTER_ID $WORKING_BUCKET
    function cleanup() {
        delete_cluster ${DATAPROC_CLUSTER_ID}
    }
    trap cleanup EXIT
fi

function run_spark_submit() {
    submit $DATAPROC_CLUSTER_ID $WORKING_BUCKET "$@"
}

function collect_history() {
    # collect some history for benchmarking analysis and performance debugging
    local artifact_bucket=$ARTIFACT_BUCKET
    local cluster_id=$DATAPROC_CLUSTER_ID
    local workdir="working"

    mkdir -p $workdir/logs
    gsutil ls -rl "$WORKING_BUCKET/**" >$workdir/logs/bucket-listing.txt
    gcloud beta dataproc clusters describe $cluster_id --format json >$workdir/logs/dataproc-clusters-describe.json
    gcloud beta dataproc jobs list --cluster $cluster_id --format json >$workdir/logs/dataproc-jobs-list.json
    gsutil -m rsync -r $workdir/ $artifact_bucket/$cluster_id/
    # copy spark history and yarn logs
    cluster_uuid=$(jq -r '.clusterUuid' working/logs/dataproc-clusters-describe.json)
    gsutil -m rsync -r "gs://dataproc-temp*/${cluster_uuid}" $artifact_bucket/$cluster_id/logs

    # copy the raw and processed data for potential analysis
    gsutil -m rsync -r $WORKING_BUCKET/client/ $artifact_bucket/$cluster_id/client/
    gsutil -m rsync -r $SERVER_A_BUCKET/processed/ $artifact_bucket/$cluster_id/server_a/processed/
    gsutil -m rsync -r $SERVER_B_BUCKET/processed/ $artifact_bucket/$cluster_id/server_b/processed/
}

START_STAGE=${START_STAGE:-0}
SKIP_GENERATE=${SKIP_GENERATE:-"false"}
if ((START_STAGE > 0)); then
    SKIP_GENERATE="true"
fi

if [[ "$SKIP_GENERATE" == "false" ]]; then
    # generate some configuration
    key_a=$(prio keygen)
    key_b=$(prio keygen)
    shared=$(prio shared-seed)
    N_DATA=${N_DATA:-128}
    N_ROWS=${N_ROWS:-1000}
    SCALE=${SCALE:-1}
    BATCH_ID=${BATCH_ID:-"test"}

    echo $key_a | jq >"working/server_a_keys.json"
    echo $key_b | jq >"working/server_b_keys.json"
    echo $shared | jq >"working/shared_seed.json"
    cat <<EOF >working/config.json
{
    "n_data": $N_DATA,
    "batch_id": "$BATCH_ID"
}
EOF
else
    key_a=$(jq '.' working/server_a_keys.json)
    key_b=$(jq '.' working/server_b_keys.json)
    shared=$(jq '.' working/shared_seed.json)
fi

N_DATA=$(jq -r ".n_data" working/config.json)
BATCH_ID=$(jq -r ".batch_id" working/config.json)
SHARED_SECRET=${SHARED_SECRET:-$(jq -r ".shared_seed" <<<$shared)}
SERVER_A_PUBLIC_KEY=${SERVER_A_PUBLIC_KEY:-$(jq -r ".public_key" <<<$key_a)}
SERVER_B_PUBLIC_KEY=${SERVER_B_PUBLIC_KEY:-$(jq -r ".public_key" <<<$key_b)}
SERVER_A_PRIVATE_KEY=${SERVER_A_PRIVATE_KEY:-$(jq -r ".private_key" <<<$key_a)}
SERVER_B_PRIVATE_KEY=${SERVER_B_PRIVATE_KEY:-$(jq -r ".private_key" <<<$key_b)}

if [[ "$SKIP_GENERATE" == "false" ]]; then
    time run_spark_submit generate \
        --n-data ${N_DATA} \
        --batch-id ${BATCH_ID} \
        --public-key-hex-internal ${SERVER_A_PUBLIC_KEY} \
        --public-key-hex-external ${SERVER_B_PUBLIC_KEY} \
        --output ${CLIENT_BUCKET} \
        --n-rows ${N_ROWS} \
        --scale ${SCALE}

    gsutil -m rsync -r -d ${CLIENT_BUCKET}/server_id=a/ ${SERVER_A_BUCKET}/raw/
    gsutil -m rsync -r -d ${CLIENT_BUCKET}/server_id=b/ ${SERVER_B_BUCKET}/raw/
fi

###########################################################
# verify1
###########################################################

if ((START_STAGE < 2)); then
    time run_spark_submit verify1 \
        --n-data ${N_DATA} \
        --batch-id ${BATCH_ID} \
        --server-id A \
        --private-key-hex ${SERVER_A_PRIVATE_KEY} \
        --shared-secret $SHARED_SECRET \
        --public-key-hex-internal ${SERVER_A_PUBLIC_KEY} \
        --public-key-hex-external ${SERVER_B_PUBLIC_KEY} \
        --input ${SERVER_A_BUCKET}/raw \
        --output ${SERVER_A_BUCKET}/intermediate/internal/verify1

    # NOTE: we ensure these buckets are completely synchronized, otherwise it is
    # possible to for the job to fail during the publishing step.
    gsutil -m rsync -r -d \
        ${SERVER_A_BUCKET}/intermediate/internal/verify1/ \
        ${SERVER_B_BUCKET}/intermediate/external/verify1/

    time run_spark_submit verify1 \
        --n-data ${N_DATA} \
        --batch-id ${BATCH_ID} \
        --server-id B \
        --private-key-hex ${SERVER_B_PRIVATE_KEY} \
        --shared-secret $SHARED_SECRET \
        --public-key-hex-internal ${SERVER_B_PUBLIC_KEY} \
        --public-key-hex-external ${SERVER_A_PUBLIC_KEY} \
        --input ${SERVER_B_BUCKET}/raw \
        --output ${SERVER_B_BUCKET}/intermediate/internal/verify1

    gsutil -m rsync -r -d \
        ${SERVER_B_BUCKET}/intermediate/internal/verify1/ \
        ${SERVER_A_BUCKET}/intermediate/external/verify1/
fi

###########################################################
# verify2
###########################################################

if ((START_STAGE < 3)); then
    time run_spark_submit verify2 \
        --n-data ${N_DATA} \
        --batch-id ${BATCH_ID} \
        --server-id A \
        --private-key-hex ${SERVER_A_PRIVATE_KEY} \
        --shared-secret $SHARED_SECRET \
        --public-key-hex-internal ${SERVER_A_PUBLIC_KEY} \
        --public-key-hex-external ${SERVER_B_PUBLIC_KEY} \
        --input ${SERVER_A_BUCKET}/raw \
        --input-internal ${SERVER_A_BUCKET}/intermediate/internal/verify1 \
        --input-external ${SERVER_A_BUCKET}/intermediate/external/verify1 \
        --output ${SERVER_A_BUCKET}/intermediate/internal/verify2/

    gsutil -m rsync -r -d \
        ${SERVER_A_BUCKET}/intermediate/internal/verify2/ \
        ${SERVER_B_BUCKET}/intermediate/external/verify2/

    time run_spark_submit verify2 \
        --n-data ${N_DATA} \
        --batch-id ${BATCH_ID} \
        --server-id B \
        --private-key-hex ${SERVER_B_PRIVATE_KEY} \
        --shared-secret $SHARED_SECRET \
        --public-key-hex-internal ${SERVER_B_PUBLIC_KEY} \
        --public-key-hex-external ${SERVER_A_PUBLIC_KEY} \
        --input ${SERVER_B_BUCKET}/raw \
        --input-internal ${SERVER_B_BUCKET}/intermediate/internal/verify1/ \
        --input-external ${SERVER_B_BUCKET}/intermediate/external/verify1/ \
        --output ${SERVER_B_BUCKET}/intermediate/internal/verify2/

    gsutil -m rsync -r -d \
        ${SERVER_B_BUCKET}/intermediate/internal/verify2/ \
        ${SERVER_A_BUCKET}/intermediate/external/verify2/
fi
###########################################################
# aggregate
###########################################################

if ((START_STAGE < 4)); then
    time run_spark_submit aggregate \
        --n-data ${N_DATA} \
        --batch-id ${BATCH_ID} \
        --server-id A \
        --private-key-hex ${SERVER_A_PRIVATE_KEY} \
        --shared-secret $SHARED_SECRET \
        --public-key-hex-internal ${SERVER_A_PUBLIC_KEY} \
        --public-key-hex-external ${SERVER_B_PUBLIC_KEY} \
        --input ${SERVER_A_BUCKET}/raw \
        --input-internal ${SERVER_A_BUCKET}/intermediate/internal/verify2 \
        --input-external ${SERVER_A_BUCKET}/intermediate/external/verify2 \
        --output ${SERVER_A_BUCKET}/intermediate/internal/aggregate

    gsutil -m rsync -r -d \
        ${SERVER_A_BUCKET}/intermediate/internal/aggregate/ \
        ${SERVER_B_BUCKET}/intermediate/external/aggregate/

    time run_spark_submit aggregate \
        --n-data ${N_DATA} \
        --batch-id ${BATCH_ID} \
        --server-id B \
        --private-key-hex ${SERVER_B_PRIVATE_KEY} \
        --shared-secret $SHARED_SECRET \
        --public-key-hex-internal ${SERVER_B_PUBLIC_KEY} \
        --public-key-hex-external ${SERVER_A_PUBLIC_KEY} \
        --input ${SERVER_B_BUCKET}/raw \
        --input-internal ${SERVER_B_BUCKET}/intermediate/internal/verify2 \
        --input-external ${SERVER_B_BUCKET}/intermediate/external/verify2 \
        --output ${SERVER_B_BUCKET}/intermediate/internal/aggregate

    gsutil -m rsync -r -d \
        ${SERVER_B_BUCKET}/intermediate/internal/aggregate/ \
        ${SERVER_A_BUCKET}/intermediate/external/aggregate/
fi

###########################################################
# publish
###########################################################

time run_spark_submit publish \
    --n-data ${N_DATA} \
    --batch-id ${BATCH_ID} \
    --server-id A \
    --private-key-hex ${SERVER_A_PRIVATE_KEY} \
    --shared-secret $SHARED_SECRET \
    --public-key-hex-internal ${SERVER_A_PUBLIC_KEY} \
    --public-key-hex-external ${SERVER_B_PUBLIC_KEY} \
    --input-internal ${SERVER_A_BUCKET}/intermediate/internal/aggregate/*.json \
    --input-external ${SERVER_A_BUCKET}/intermediate/external/aggregate/*.json \
    --output ${SERVER_A_BUCKET}/processed

gsutil -m rsync -r -d ${SERVER_A_BUCKET}/processed/ working/server_a/processed/
jq '.' working/server_a/processed/*.json
[[ $(jq ".error" working/server_a/processed/*.json) -eq 0 ]]

time run_spark_submit publish \
    --n-data ${N_DATA} \
    --batch-id ${BATCH_ID} \
    --server-id B \
    --private-key-hex ${SERVER_B_PRIVATE_KEY} \
    --shared-secret $SHARED_SECRET \
    --public-key-hex-internal ${SERVER_B_PUBLIC_KEY} \
    --public-key-hex-external ${SERVER_A_PUBLIC_KEY} \
    --input-internal ${SERVER_B_BUCKET}/intermediate/internal/aggregate/*.json \
    --input-external ${SERVER_B_BUCKET}/intermediate/external/aggregate/*.json \
    --output ${SERVER_B_BUCKET}/processed

gsutil -m rsync -r -d ${SERVER_B_BUCKET}/processed/ working/server_b/processed/
jq '.' working/server_b/processed/*.json
[[ $(jq ".error" working/server_b/processed/*.json) -eq 0 ]]

collect_history
